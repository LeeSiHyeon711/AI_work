{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = 10\n",
    "b = 20\n",
    "print(a+b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "토치 다운로드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in c:\\python312\\lib\\site-packages (2.5.1)\n",
      "Requirement already satisfied: filelock in c:\\python312\\lib\\site-packages (from torch) (3.16.1)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in c:\\python312\\lib\\site-packages (from torch) (4.12.2)\n",
      "Requirement already satisfied: networkx in c:\\python312\\lib\\site-packages (from torch) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in c:\\python312\\lib\\site-packages (from torch) (3.1.4)\n",
      "Requirement already satisfied: fsspec in c:\\python312\\lib\\site-packages (from torch) (2024.10.0)\n",
      "Requirement already satisfied: setuptools in c:\\python312\\lib\\site-packages (from torch) (75.6.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in c:\\python312\\lib\\site-packages (from torch) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\python312\\lib\\site-packages (from sympy==1.13.1->torch) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\python312\\lib\\site-packages (from jinja2->torch) (3.0.2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.2 -> 24.3.1\n",
      "[notice] To update, run: C:\\Python312\\python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "! pip install torch\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "토치 import 안될때 추가 다운로드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting torch\n",
      "  Using cached torch-2.5.1-cp311-cp311-win_amd64.whl.metadata (28 kB)\n",
      "Collecting filelock (from torch)\n",
      "  Using cached filelock-3.16.1-py3-none-any.whl.metadata (2.9 kB)\n",
      "Collecting typing-extensions>=4.8.0 (from torch)\n",
      "  Using cached typing_extensions-4.12.2-py3-none-any.whl.metadata (3.0 kB)\n",
      "Collecting networkx (from torch)\n",
      "  Using cached networkx-3.4.2-py3-none-any.whl.metadata (6.3 kB)\n",
      "Collecting jinja2 (from torch)\n",
      "  Using cached jinja2-3.1.4-py3-none-any.whl.metadata (2.6 kB)\n",
      "Collecting fsspec (from torch)\n",
      "  Using cached fsspec-2024.10.0-py3-none-any.whl.metadata (11 kB)\n",
      "Collecting sympy==1.13.1 (from torch)\n",
      "  Using cached sympy-1.13.1-py3-none-any.whl.metadata (12 kB)\n",
      "Collecting mpmath<1.4,>=1.1.0 (from sympy==1.13.1->torch)\n",
      "  Using cached mpmath-1.3.0-py3-none-any.whl.metadata (8.6 kB)\n",
      "Collecting MarkupSafe>=2.0 (from jinja2->torch)\n",
      "  Using cached MarkupSafe-3.0.2-cp311-cp311-win_amd64.whl.metadata (4.1 kB)\n",
      "Using cached torch-2.5.1-cp311-cp311-win_amd64.whl (203.1 MB)\n",
      "Using cached sympy-1.13.1-py3-none-any.whl (6.2 MB)\n",
      "Using cached typing_extensions-4.12.2-py3-none-any.whl (37 kB)\n",
      "Using cached filelock-3.16.1-py3-none-any.whl (16 kB)\n",
      "Using cached fsspec-2024.10.0-py3-none-any.whl (179 kB)\n",
      "Using cached jinja2-3.1.4-py3-none-any.whl (133 kB)\n",
      "Using cached networkx-3.4.2-py3-none-any.whl (1.7 MB)\n",
      "Using cached MarkupSafe-3.0.2-cp311-cp311-win_amd64.whl (15 kB)\n",
      "Using cached mpmath-1.3.0-py3-none-any.whl (536 kB)\n",
      "Installing collected packages: mpmath, typing-extensions, sympy, networkx, MarkupSafe, fsspec, filelock, jinja2, torch\n",
      "  Attempting uninstall: mpmath\n",
      "    Found existing installation: mpmath 1.3.0\n",
      "    Uninstalling mpmath-1.3.0:\n",
      "      Successfully uninstalled mpmath-1.3.0\n",
      "  Attempting uninstall: typing-extensions\n",
      "    Found existing installation: typing_extensions 4.12.2\n",
      "    Uninstalling typing_extensions-4.12.2:\n",
      "      Successfully uninstalled typing_extensions-4.12.2\n",
      "  Attempting uninstall: sympy\n",
      "    Found existing installation: sympy 1.13.1\n",
      "    Uninstalling sympy-1.13.1:\n",
      "      Successfully uninstalled sympy-1.13.1\n",
      "  Attempting uninstall: networkx\n",
      "    Found existing installation: networkx 3.4.2\n",
      "    Uninstalling networkx-3.4.2:\n",
      "      Successfully uninstalled networkx-3.4.2\n",
      "  Attempting uninstall: MarkupSafe\n",
      "    Found existing installation: MarkupSafe 3.0.2\n",
      "    Uninstalling MarkupSafe-3.0.2:\n",
      "      Successfully uninstalled MarkupSafe-3.0.2\n",
      "  Attempting uninstall: fsspec\n",
      "    Found existing installation: fsspec 2024.10.0\n",
      "    Uninstalling fsspec-2024.10.0:\n",
      "      Successfully uninstalled fsspec-2024.10.0\n",
      "  Attempting uninstall: filelock\n",
      "    Found existing installation: filelock 3.16.1\n",
      "    Uninstalling filelock-3.16.1:\n",
      "      Successfully uninstalled filelock-3.16.1\n",
      "  Attempting uninstall: jinja2\n",
      "    Found existing installation: Jinja2 3.1.4\n",
      "    Uninstalling Jinja2-3.1.4:\n",
      "      Successfully uninstalled Jinja2-3.1.4\n",
      "  Attempting uninstall: torch\n",
      "    Found existing installation: torch 2.5.1\n",
      "    Uninstalling torch-2.5.1:\n",
      "      Successfully uninstalled torch-2.5.1\n",
      "Successfully installed MarkupSafe-3.0.2 filelock-3.16.1 fsspec-2024.10.0 jinja2-3.1.4 mpmath-1.3.0 networkx-3.4.2 sympy-1.13.1 torch-2.5.1 typing-extensions-4.12.2\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  WARNING: The script isympy.exe is installed in 'c:\\Users\\COM\\AppData\\Local\\Programs\\Python\\Python311\\Scripts' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n",
      "  WARNING: The scripts convert-caffe2-to-onnx.exe, convert-onnx-to-caffe2.exe, torchfrtrace.exe and torchrun.exe are installed in 'c:\\Users\\COM\\AppData\\Local\\Programs\\Python\\Python311\\Scripts' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n",
      "\n",
      "[notice] A new release of pip is available: 24.0 -> 24.3.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "pip install --upgrade --force-reinstall torch\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 파이토치 기초 문법"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1, 2, 3])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "t1 = torch.tensor([1,2,3])\n",
    "t1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "tensor 만들기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t1 = torch.tensor([[1,2],[4,5]])\n",
    "# t2 = torch.tensor([[1,2],[3,4]], device = \"cuda:0\")\n",
    "t3 = torch.tensor([[1,2],[3,4]], dtype = torch.float64)\n",
    "\n",
    "print(t1)\n",
    "# print(t2)\n",
    "print(t3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf = torch.FloatTensor([[1,2],[3,4]])\n",
    "tl = torch.LongTensor([[1,2],[3,4]])\n",
    "tB = torch.ByteTensor([[1,2],[3,4]])\n",
    "\n",
    "print(tf)\n",
    "print(tl)\n",
    "print(tB)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 텐서 다루기 - 넘파이 호환"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "x = np.array([[1,2],[3,4]])\n",
    "print(x, type(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.from_numpy(x)\n",
    "print(x, type(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = x.numpy()\n",
    "print(x, type(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "텐서 크기 구하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x=np.arange(1,13).reshape(3,2,2)\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tx=torch.from_numpy(x)\n",
    "tx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"numpy 배열 크기\", x.shape)\n",
    "print(\"tensor 크기\", tx.shape)\n",
    "print(tx.size())\n",
    "print(tx.shape[0])\n",
    "print(tx.size(0))\n",
    "print(tx.shape[1])\n",
    "print(tx.size(1))\n",
    "print(tx.shape[2])\n",
    "print(tx.size(2))\n",
    "# 뒷자리에서 부터 확인할 경우 마이너스를 붙인다\n",
    "print(tx.size(-1))\n",
    "# tx의 차원수를 구한다\n",
    "print(tx.dim())\n",
    "print(len(tx.size()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 텐서의 연산"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a=np.arange(1,5).reshape(2,2)\n",
    "b=np.arange(2,10,2).reshape(2,2)\n",
    "print(a)\n",
    "print(b)\n",
    "\n",
    "# 텐서로 변환\n",
    "ta=torch.from_numpy(a)\n",
    "tb=torch.from_numpy(b)\n",
    "print(ta)\n",
    "print(tb)\n",
    "\n",
    "# 더하기\n",
    "print(ta+tb)\n",
    "\n",
    "# 빼기\n",
    "print(ta-tb)\n",
    "\n",
    "# 곱하기\n",
    "print(ta*tb)\n",
    "\n",
    "# 나누기\n",
    "print(ta/tb)\n",
    "\n",
    "# 나누기 값을 정수로 변환\n",
    "print(ta//tb)\n",
    "\n",
    "# 각 요소가 같은지 비교\n",
    "print(ta==tb)\n",
    "\n",
    "# 각 요소가 다른지 비교\n",
    "print(ta!=tb)\n",
    "\n",
    "# ta의 각 요소를 tb로 제곱\n",
    "print(ta**tb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 인플레이스 연산"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 언더바를 붙여주면 연산한 값이 새롭게 저장된다\n",
    "# ta = ta+tb\n",
    "print(ta)\n",
    "print(ta.mul_(tb))\n",
    "print(ta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x=np.arange(1,10).reshape(3,3)\n",
    "tx=torch.from_numpy(x)\n",
    "print(tx.sum())\n",
    "# print(tx.mean())\n",
    "print(tx.sum(dim=0))\n",
    "print(tx.sum(dim=-1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 브로드캐스트 연산"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 스칼라와 텐스의 연산\n",
    "x=np.arange(1,5).reshape(2,2)\n",
    "t1=torch.from_numpy(x)\n",
    "y=1\n",
    "print(t1)\n",
    "print(t1+y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 텐서와 벡터의 연산\n",
    "t2=torch.tensor([1,2])\n",
    "print(t1)\n",
    "print(t2)\n",
    "print(t1+t2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1차원의 갯수가 같지 않으면 연산이 불가능하다(아래는 안되는 예제)\n",
    "t3=np.arange(1,7).reshape(2,3)\n",
    "print(t3)\n",
    "print(t1)\n",
    "print(t1+t3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 텐서의 형태 변환"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3차원 배열을 준비\n",
    "x=np.arange(1,13).reshape(3,2,2)\n",
    "\n",
    "# 텐서로 전환\n",
    "x=torch.from_numpy(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# view 함수를 이용하여 텐서 형태 변환하기\n",
    "print(x.view(3,4))\n",
    "print(x.view(4,3))\n",
    "print(x.view(6,2))\n",
    "\n",
    "# 애매한 포지션의 경우 -1을 붙이면 알아서 정하라는 뜻이 됨\n",
    "print(x.view(3,-1,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 다양한 모양으로 변환 가능\n",
    "print(x.view(3,4))\n",
    "print(x.view(3,1,4))\n",
    "print(x.view(-1))\n",
    "print(x.view(3,-1))\n",
    "print(x.view(-1,1,4))\n",
    "print(x.view(3,2,2,-1))\n",
    "\n",
    "# reshape 동일한 역할\n",
    "print(x.reshape(3,4))\n",
    "print(x.reshape(3,1,4))\n",
    "print(x.reshape(-1))\n",
    "print(x.reshape(3,-1))\n",
    "print(x.reshape(-1,1,4))\n",
    "print(x.reshape(3,2,2,-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x1=x.view(3,-1,4)\n",
    "x2=x1.squeeze()\n",
    "print(x2)\n",
    "x3=x2.unsqueeze(1)\n",
    "print(x3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 텐서 자르기 & 붙이기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(x)\n",
    "print(x.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(x[0])\n",
    "print(x[1])\n",
    "print(x[2])         # print(x[-1])과 같은 결과"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 이 3가지 출력문은 모두 같다\n",
    "print(x[0])\n",
    "print(x[0,:])\n",
    "print(x[0,:,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(x[-1])\n",
    "print(x[-1,:])\n",
    "print(x[-1,:,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(x[:,0,:])\n",
    "print(x[:,:,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(x[1:3,:,:])\n",
    "print(x[:,1,:])\n",
    "print(x[:,:-1,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x=np.arange(1,101).reshape(10,10)\n",
    "# x=torch.FloatTensor(10,4)\n",
    "print(x)\n",
    "x=torch.from_numpy(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4개씩 자르기(split)\n",
    "splits=x.split(4, dim=1)\n",
    "for s in splits:\n",
    "    print(s.size())\n",
    "    print(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# chunk 사용하기\n",
    "chunks=x.chunk(4,dim=0)\n",
    "for c in chunks:\n",
    "    print(c)\n",
    "    print(c.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x=np.arange(1,19).reshape(2,3,3)\n",
    "x=torch.from_numpy(x)\n",
    "print(x)\n",
    "\n",
    "indice=torch.LongTensor([2,0])\n",
    "y=x.index_select(dim=1, index=indice)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### stack 으로 붙이기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a=np.arange(1,7).reshape(2,3)\n",
    "a=torch.from_numpy(a)\n",
    "print(a)\n",
    "\n",
    "b=np.arange(7,13).reshape(2,3)\n",
    "b=torch.from_numpy(b)\n",
    "print(b)\n",
    "\n",
    "# 2차원 a,b를 합쳐서 3차원 c로 만들기\n",
    "c=torch.stack([a,b])\n",
    "print(c)\n",
    "\n",
    "# 1행의 내용을 열로하여 붙이기\n",
    "d=torch.stack([a,b], dim=-1)\n",
    "print(d)\n",
    "\n",
    "# cat함수는 차원증가없이 붙일수 있음\n",
    "e=torch.cat([a,b])\n",
    "print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 특정 차원을 여러개 만들어주는 expand"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a1=a.view(2,1,3)\n",
    "print(a1)\n",
    "\n",
    "a2=a1.expand(2,3,3)\n",
    "print(a2)\n",
    "\n",
    "print(torch.cat([a1]*3, dim=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 유용한 함수들\n",
    "- random\n",
    "- argmax\n",
    "- topk\n",
    "- masked\n",
    "- ones\n",
    "- zeros\n",
    "- ones_like\n",
    "- zeros_like"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.random\n",
    "\n",
    "x=torch.randperm(100)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "values, indices=torch.topk(x, k=2, dim=-1)\n",
    "print(values)\n",
    "print(indice)\n",
    "print(values.size())\n",
    "print(indices.size())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
