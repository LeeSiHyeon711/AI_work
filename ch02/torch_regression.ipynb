{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.],\n",
      "        [2.],\n",
      "        [3.],\n",
      "        [4.],\n",
      "        [5.]])\n",
      "torch.Size([5, 1])\n",
      "tensor([[ 2.],\n",
      "        [ 4.],\n",
      "        [ 6.],\n",
      "        [ 8.],\n",
      "        [10.]])\n",
      "torch.Size([5, 1])\n"
     ]
    }
   ],
   "source": [
    "# 재실행 후에도 동일한 결과가 나오도록 랜덤 시드를 설정\n",
    "torch.manual_seed(1)\n",
    "\n",
    "# 변수 선언\n",
    "x_train=torch.FloatTensor([[1],[2],[3],[4],[5]])\n",
    "y_train=torch.FloatTensor([[2],[4],[6],[8],[10]])\n",
    "\n",
    "# 선언한 변수와 변수 형태 확인\n",
    "print(x_train)\n",
    "print(x_train.shape)\n",
    "print(y_train)\n",
    "print(y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.], requires_grad=True)\n",
      "tensor([0.], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "w=torch.zeros(1, requires_grad=True)\n",
    "print(w)\n",
    "b=torch.zeros(1, requires_grad=True)\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.]], grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# 가설 만들기\n",
    "h=x_train*w+b\n",
    "print(h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(44., grad_fn=<MeanBackward0>)\n"
     ]
    }
   ],
   "source": [
    "cost=torch.mean((h-y_train)**2)\n",
    "print(cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer=optim.SGD([w, b], lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer.zero_grad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "cost.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch    0/20000 w:2.000 b:0.006 cost:0.000036\n",
      "Epoch  100/20000 w:2.000 b:0.001 cost:0.000001\n",
      "Epoch  200/20000 w:2.000 b:0.000 cost:0.000000\n",
      "Epoch  300/20000 w:2.000 b:0.000 cost:0.000000\n",
      "Epoch  400/20000 w:2.000 b:0.000 cost:0.000000\n",
      "Epoch  500/20000 w:2.000 b:0.000 cost:0.000000\n",
      "Epoch  600/20000 w:2.000 b:0.000 cost:0.000000\n",
      "Epoch  700/20000 w:2.000 b:0.000 cost:0.000000\n",
      "Epoch  800/20000 w:2.000 b:0.000 cost:0.000000\n",
      "Epoch  900/20000 w:2.000 b:0.000 cost:0.000000\n",
      "Epoch 1000/20000 w:2.000 b:0.000 cost:0.000000\n",
      "Epoch 1100/20000 w:2.000 b:0.000 cost:0.000000\n",
      "Epoch 1200/20000 w:2.000 b:0.000 cost:0.000000\n",
      "Epoch 1300/20000 w:2.000 b:0.000 cost:0.000000\n",
      "Epoch 1400/20000 w:2.000 b:0.000 cost:0.000000\n",
      "Epoch 1500/20000 w:2.000 b:0.000 cost:0.000000\n",
      "Epoch 1600/20000 w:2.000 b:0.000 cost:0.000000\n",
      "Epoch 1700/20000 w:2.000 b:0.000 cost:0.000000\n",
      "Epoch 1800/20000 w:2.000 b:0.000 cost:0.000000\n",
      "Epoch 1900/20000 w:2.000 b:0.000 cost:0.000000\n",
      "Epoch 2000/20000 w:2.000 b:0.000 cost:0.000000\n",
      "Epoch 2100/20000 w:2.000 b:0.000 cost:0.000000\n",
      "Epoch 2200/20000 w:2.000 b:0.000 cost:0.000000\n",
      "Epoch 2300/20000 w:2.000 b:0.000 cost:0.000000\n",
      "Epoch 2400/20000 w:2.000 b:0.000 cost:0.000000\n",
      "Epoch 2500/20000 w:2.000 b:0.000 cost:0.000000\n",
      "Epoch 2600/20000 w:2.000 b:0.000 cost:0.000000\n",
      "Epoch 2700/20000 w:2.000 b:0.000 cost:0.000000\n",
      "Epoch 2800/20000 w:2.000 b:0.000 cost:0.000000\n",
      "Epoch 2900/20000 w:2.000 b:0.000 cost:0.000000\n",
      "Epoch 3000/20000 w:2.000 b:0.000 cost:0.000000\n",
      "Epoch 3100/20000 w:2.000 b:0.000 cost:0.000000\n",
      "Epoch 3200/20000 w:2.000 b:0.000 cost:0.000000\n",
      "Epoch 3300/20000 w:2.000 b:0.000 cost:0.000000\n",
      "Epoch 3400/20000 w:2.000 b:0.000 cost:0.000000\n",
      "Epoch 3500/20000 w:2.000 b:0.000 cost:0.000000\n",
      "Epoch 3600/20000 w:2.000 b:0.000 cost:0.000000\n",
      "Epoch 3700/20000 w:2.000 b:0.000 cost:0.000000\n",
      "Epoch 3800/20000 w:2.000 b:0.000 cost:0.000000\n",
      "Epoch 3900/20000 w:2.000 b:0.000 cost:0.000000\n",
      "Epoch 4000/20000 w:2.000 b:0.000 cost:0.000000\n",
      "Epoch 4100/20000 w:2.000 b:0.000 cost:0.000000\n",
      "Epoch 4200/20000 w:2.000 b:0.000 cost:0.000000\n",
      "Epoch 4300/20000 w:2.000 b:0.000 cost:0.000000\n",
      "Epoch 4400/20000 w:2.000 b:0.000 cost:0.000000\n",
      "Epoch 4500/20000 w:2.000 b:0.000 cost:0.000000\n",
      "Epoch 4600/20000 w:2.000 b:0.000 cost:0.000000\n",
      "Epoch 4700/20000 w:2.000 b:0.000 cost:0.000000\n",
      "Epoch 4800/20000 w:2.000 b:0.000 cost:0.000000\n",
      "Epoch 4900/20000 w:2.000 b:0.000 cost:0.000000\n",
      "Epoch 5000/20000 w:2.000 b:0.000 cost:0.000000\n",
      "Epoch 5100/20000 w:2.000 b:0.000 cost:0.000000\n",
      "Epoch 5200/20000 w:2.000 b:0.000 cost:0.000000\n",
      "Epoch 5300/20000 w:2.000 b:0.000 cost:0.000000\n",
      "Epoch 5400/20000 w:2.000 b:0.000 cost:0.000000\n",
      "Epoch 5500/20000 w:2.000 b:0.000 cost:0.000000\n",
      "Epoch 5600/20000 w:2.000 b:0.000 cost:0.000000\n",
      "Epoch 5700/20000 w:2.000 b:0.000 cost:0.000000\n",
      "Epoch 5800/20000 w:2.000 b:0.000 cost:0.000000\n",
      "Epoch 5900/20000 w:2.000 b:0.000 cost:0.000000\n",
      "Epoch 6000/20000 w:2.000 b:0.000 cost:0.000000\n",
      "Epoch 6100/20000 w:2.000 b:0.000 cost:0.000000\n",
      "Epoch 6200/20000 w:2.000 b:0.000 cost:0.000000\n",
      "Epoch 6300/20000 w:2.000 b:0.000 cost:0.000000\n",
      "Epoch 6400/20000 w:2.000 b:0.000 cost:0.000000\n",
      "Epoch 6500/20000 w:2.000 b:0.000 cost:0.000000\n",
      "Epoch 6600/20000 w:2.000 b:0.000 cost:0.000000\n",
      "Epoch 6700/20000 w:2.000 b:0.000 cost:0.000000\n",
      "Epoch 6800/20000 w:2.000 b:0.000 cost:0.000000\n",
      "Epoch 6900/20000 w:2.000 b:0.000 cost:0.000000\n",
      "Epoch 7000/20000 w:2.000 b:0.000 cost:0.000000\n",
      "Epoch 7100/20000 w:2.000 b:0.000 cost:0.000000\n",
      "Epoch 7200/20000 w:2.000 b:0.000 cost:0.000000\n",
      "Epoch 7300/20000 w:2.000 b:0.000 cost:0.000000\n",
      "Epoch 7400/20000 w:2.000 b:0.000 cost:0.000000\n",
      "Epoch 7500/20000 w:2.000 b:0.000 cost:0.000000\n",
      "Epoch 7600/20000 w:2.000 b:0.000 cost:0.000000\n",
      "Epoch 7700/20000 w:2.000 b:0.000 cost:0.000000\n",
      "Epoch 7800/20000 w:2.000 b:0.000 cost:0.000000\n",
      "Epoch 7900/20000 w:2.000 b:0.000 cost:0.000000\n",
      "Epoch 8000/20000 w:2.000 b:0.000 cost:0.000000\n",
      "Epoch 8100/20000 w:2.000 b:0.000 cost:0.000000\n",
      "Epoch 8200/20000 w:2.000 b:0.000 cost:0.000000\n",
      "Epoch 8300/20000 w:2.000 b:0.000 cost:0.000000\n",
      "Epoch 8400/20000 w:2.000 b:0.000 cost:0.000000\n",
      "Epoch 8500/20000 w:2.000 b:0.000 cost:0.000000\n",
      "Epoch 8600/20000 w:2.000 b:0.000 cost:0.000000\n",
      "Epoch 8700/20000 w:2.000 b:0.000 cost:0.000000\n",
      "Epoch 8800/20000 w:2.000 b:0.000 cost:0.000000\n",
      "Epoch 8900/20000 w:2.000 b:0.000 cost:0.000000\n",
      "Epoch 9000/20000 w:2.000 b:0.000 cost:0.000000\n",
      "Epoch 9100/20000 w:2.000 b:0.000 cost:0.000000\n",
      "Epoch 9200/20000 w:2.000 b:0.000 cost:0.000000\n",
      "Epoch 9300/20000 w:2.000 b:0.000 cost:0.000000\n",
      "Epoch 9400/20000 w:2.000 b:0.000 cost:0.000000\n",
      "Epoch 9500/20000 w:2.000 b:0.000 cost:0.000000\n",
      "Epoch 9600/20000 w:2.000 b:0.000 cost:0.000000\n",
      "Epoch 9700/20000 w:2.000 b:0.000 cost:0.000000\n",
      "Epoch 9800/20000 w:2.000 b:0.000 cost:0.000000\n",
      "Epoch 9900/20000 w:2.000 b:0.000 cost:0.000000\n",
      "Epoch 10000/20000 w:2.000 b:0.000 cost:0.000000\n",
      "Epoch 10100/20000 w:2.000 b:0.000 cost:0.000000\n",
      "Epoch 10200/20000 w:2.000 b:0.000 cost:0.000000\n",
      "Epoch 10300/20000 w:2.000 b:0.000 cost:0.000000\n",
      "Epoch 10400/20000 w:2.000 b:0.000 cost:0.000000\n",
      "Epoch 10500/20000 w:2.000 b:0.000 cost:0.000000\n",
      "Epoch 10600/20000 w:2.000 b:0.000 cost:0.000000\n",
      "Epoch 10700/20000 w:2.000 b:0.000 cost:0.000000\n",
      "Epoch 10800/20000 w:2.000 b:0.000 cost:0.000000\n",
      "Epoch 10900/20000 w:2.000 b:0.000 cost:0.000000\n",
      "Epoch 11000/20000 w:2.000 b:0.000 cost:0.000000\n",
      "Epoch 11100/20000 w:2.000 b:0.000 cost:0.000000\n",
      "Epoch 11200/20000 w:2.000 b:0.000 cost:0.000000\n",
      "Epoch 11300/20000 w:2.000 b:0.000 cost:0.000000\n",
      "Epoch 11400/20000 w:2.000 b:0.000 cost:0.000000\n",
      "Epoch 11500/20000 w:2.000 b:0.000 cost:0.000000\n",
      "Epoch 11600/20000 w:2.000 b:0.000 cost:0.000000\n",
      "Epoch 11700/20000 w:2.000 b:0.000 cost:0.000000\n",
      "Epoch 11800/20000 w:2.000 b:0.000 cost:0.000000\n",
      "Epoch 11900/20000 w:2.000 b:0.000 cost:0.000000\n",
      "Epoch 12000/20000 w:2.000 b:0.000 cost:0.000000\n",
      "Epoch 12100/20000 w:2.000 b:0.000 cost:0.000000\n",
      "Epoch 12200/20000 w:2.000 b:0.000 cost:0.000000\n",
      "Epoch 12300/20000 w:2.000 b:0.000 cost:0.000000\n",
      "Epoch 12400/20000 w:2.000 b:0.000 cost:0.000000\n",
      "Epoch 12500/20000 w:2.000 b:0.000 cost:0.000000\n",
      "Epoch 12600/20000 w:2.000 b:0.000 cost:0.000000\n",
      "Epoch 12700/20000 w:2.000 b:0.000 cost:0.000000\n",
      "Epoch 12800/20000 w:2.000 b:0.000 cost:0.000000\n",
      "Epoch 12900/20000 w:2.000 b:0.000 cost:0.000000\n",
      "Epoch 13000/20000 w:2.000 b:0.000 cost:0.000000\n",
      "Epoch 13100/20000 w:2.000 b:0.000 cost:0.000000\n",
      "Epoch 13200/20000 w:2.000 b:0.000 cost:0.000000\n",
      "Epoch 13300/20000 w:2.000 b:0.000 cost:0.000000\n",
      "Epoch 13400/20000 w:2.000 b:0.000 cost:0.000000\n",
      "Epoch 13500/20000 w:2.000 b:0.000 cost:0.000000\n",
      "Epoch 13600/20000 w:2.000 b:0.000 cost:0.000000\n",
      "Epoch 13700/20000 w:2.000 b:0.000 cost:0.000000\n",
      "Epoch 13800/20000 w:2.000 b:0.000 cost:0.000000\n",
      "Epoch 13900/20000 w:2.000 b:0.000 cost:0.000000\n",
      "Epoch 14000/20000 w:2.000 b:0.000 cost:0.000000\n",
      "Epoch 14100/20000 w:2.000 b:0.000 cost:0.000000\n",
      "Epoch 14200/20000 w:2.000 b:0.000 cost:0.000000\n",
      "Epoch 14300/20000 w:2.000 b:0.000 cost:0.000000\n",
      "Epoch 14400/20000 w:2.000 b:0.000 cost:0.000000\n",
      "Epoch 14500/20000 w:2.000 b:0.000 cost:0.000000\n",
      "Epoch 14600/20000 w:2.000 b:0.000 cost:0.000000\n",
      "Epoch 14700/20000 w:2.000 b:0.000 cost:0.000000\n",
      "Epoch 14800/20000 w:2.000 b:0.000 cost:0.000000\n",
      "Epoch 14900/20000 w:2.000 b:0.000 cost:0.000000\n",
      "Epoch 15000/20000 w:2.000 b:0.000 cost:0.000000\n",
      "Epoch 15100/20000 w:2.000 b:0.000 cost:0.000000\n",
      "Epoch 15200/20000 w:2.000 b:0.000 cost:0.000000\n",
      "Epoch 15300/20000 w:2.000 b:0.000 cost:0.000000\n",
      "Epoch 15400/20000 w:2.000 b:0.000 cost:0.000000\n",
      "Epoch 15500/20000 w:2.000 b:0.000 cost:0.000000\n",
      "Epoch 15600/20000 w:2.000 b:0.000 cost:0.000000\n",
      "Epoch 15700/20000 w:2.000 b:0.000 cost:0.000000\n",
      "Epoch 15800/20000 w:2.000 b:0.000 cost:0.000000\n",
      "Epoch 15900/20000 w:2.000 b:0.000 cost:0.000000\n",
      "Epoch 16000/20000 w:2.000 b:0.000 cost:0.000000\n",
      "Epoch 16100/20000 w:2.000 b:0.000 cost:0.000000\n",
      "Epoch 16200/20000 w:2.000 b:0.000 cost:0.000000\n",
      "Epoch 16300/20000 w:2.000 b:0.000 cost:0.000000\n",
      "Epoch 16400/20000 w:2.000 b:0.000 cost:0.000000\n",
      "Epoch 16500/20000 w:2.000 b:0.000 cost:0.000000\n",
      "Epoch 16600/20000 w:2.000 b:0.000 cost:0.000000\n",
      "Epoch 16700/20000 w:2.000 b:0.000 cost:0.000000\n",
      "Epoch 16800/20000 w:2.000 b:0.000 cost:0.000000\n",
      "Epoch 16900/20000 w:2.000 b:0.000 cost:0.000000\n",
      "Epoch 17000/20000 w:2.000 b:0.000 cost:0.000000\n",
      "Epoch 17100/20000 w:2.000 b:0.000 cost:0.000000\n",
      "Epoch 17200/20000 w:2.000 b:0.000 cost:0.000000\n",
      "Epoch 17300/20000 w:2.000 b:0.000 cost:0.000000\n",
      "Epoch 17400/20000 w:2.000 b:0.000 cost:0.000000\n",
      "Epoch 17500/20000 w:2.000 b:0.000 cost:0.000000\n",
      "Epoch 17600/20000 w:2.000 b:0.000 cost:0.000000\n",
      "Epoch 17700/20000 w:2.000 b:0.000 cost:0.000000\n",
      "Epoch 17800/20000 w:2.000 b:0.000 cost:0.000000\n",
      "Epoch 17900/20000 w:2.000 b:0.000 cost:0.000000\n",
      "Epoch 18000/20000 w:2.000 b:0.000 cost:0.000000\n",
      "Epoch 18100/20000 w:2.000 b:0.000 cost:0.000000\n",
      "Epoch 18200/20000 w:2.000 b:0.000 cost:0.000000\n",
      "Epoch 18300/20000 w:2.000 b:0.000 cost:0.000000\n",
      "Epoch 18400/20000 w:2.000 b:0.000 cost:0.000000\n",
      "Epoch 18500/20000 w:2.000 b:0.000 cost:0.000000\n",
      "Epoch 18600/20000 w:2.000 b:0.000 cost:0.000000\n",
      "Epoch 18700/20000 w:2.000 b:0.000 cost:0.000000\n",
      "Epoch 18800/20000 w:2.000 b:0.000 cost:0.000000\n",
      "Epoch 18900/20000 w:2.000 b:0.000 cost:0.000000\n",
      "Epoch 19000/20000 w:2.000 b:0.000 cost:0.000000\n",
      "Epoch 19100/20000 w:2.000 b:0.000 cost:0.000000\n",
      "Epoch 19200/20000 w:2.000 b:0.000 cost:0.000000\n",
      "Epoch 19300/20000 w:2.000 b:0.000 cost:0.000000\n",
      "Epoch 19400/20000 w:2.000 b:0.000 cost:0.000000\n",
      "Epoch 19500/20000 w:2.000 b:0.000 cost:0.000000\n",
      "Epoch 19600/20000 w:2.000 b:0.000 cost:0.000000\n",
      "Epoch 19700/20000 w:2.000 b:0.000 cost:0.000000\n",
      "Epoch 19800/20000 w:2.000 b:0.000 cost:0.000000\n",
      "Epoch 19900/20000 w:2.000 b:0.000 cost:0.000000\n"
     ]
    }
   ],
   "source": [
    "epochs=20000\n",
    "for epoch in range(epochs):\n",
    "    h=x_train*w+b\n",
    "\n",
    "    cost=torch.mean((h - y_train)**2)\n",
    "    optimizer.zero_grad()\n",
    "    cost.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if epoch % 100 == 0:\n",
    "        print('Epoch {:4d}/{} w:{:.3f} b:{:.3f} cost:{:.6f}'.format(\n",
    "            epoch, epochs, w.item(), b.item(), cost.item()\n",
    "        ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.]], grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "h=torch.FloatTensor([[7]])*w+b\n",
    "print(h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch    0/2000 W: 0.187, b: 0.080 Cost: 18.666666\n",
      "Epoch  100/2000 W: 1.746, b: 0.578 Cost: 0.048171\n",
      "Epoch  200/2000 W: 1.800, b: 0.454 Cost: 0.029767\n",
      "Epoch  300/2000 W: 1.843, b: 0.357 Cost: 0.018394\n",
      "Epoch  400/2000 W: 1.876, b: 0.281 Cost: 0.011366\n",
      "Epoch  500/2000 W: 1.903, b: 0.221 Cost: 0.007024\n",
      "Epoch  600/2000 W: 1.924, b: 0.174 Cost: 0.004340\n",
      "Epoch  700/2000 W: 1.940, b: 0.136 Cost: 0.002682\n",
      "Epoch  800/2000 W: 1.953, b: 0.107 Cost: 0.001657\n",
      "Epoch  900/2000 W: 1.963, b: 0.084 Cost: 0.001024\n",
      "Epoch 1000/2000 W: 1.971, b: 0.066 Cost: 0.000633\n",
      "Epoch 1100/2000 W: 1.977, b: 0.052 Cost: 0.000391\n",
      "Epoch 1200/2000 W: 1.982, b: 0.041 Cost: 0.000242\n",
      "Epoch 1300/2000 W: 1.986, b: 0.032 Cost: 0.000149\n",
      "Epoch 1400/2000 W: 1.989, b: 0.025 Cost: 0.000092\n",
      "Epoch 1500/2000 W: 1.991, b: 0.020 Cost: 0.000057\n",
      "Epoch 1600/2000 W: 1.993, b: 0.016 Cost: 0.000035\n",
      "Epoch 1700/2000 W: 1.995, b: 0.012 Cost: 0.000022\n",
      "Epoch 1800/2000 W: 1.996, b: 0.010 Cost: 0.000013\n",
      "Epoch 1900/2000 W: 1.997, b: 0.008 Cost: 0.000008\n",
      "Epoch 2000/2000 W: 1.997, b: 0.006 Cost: 0.000005\n"
     ]
    }
   ],
   "source": [
    "# 데이터\n",
    "x_train = torch.FloatTensor([[1],[2],[3]])\n",
    "y_train = torch.FloatTensor([[2],[4],[6]])\n",
    "\n",
    "# 모델 초기화\n",
    "W = torch.zeros(1,requires_grad=True)\n",
    "b = torch.zeros(1,requires_grad=True)\n",
    "\n",
    "# optimizer 설정\n",
    "optimizer = optim.SGD([W,b], lr=0.01)\n",
    "\n",
    "nb_epochs = 2000\n",
    "\n",
    "for epoch in range(nb_epochs+1):\n",
    "    hypothesis = x_train * W + b\n",
    "\n",
    "    cost = torch.mean((hypothesis - y_train)**2)\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    cost.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if epoch % 100 == 0:\n",
    "        print('Epoch {:4d}/{} W: {:.3f}, b: {:.3f} Cost: {:.6f}'.format(\n",
    "            epoch, nb_epochs, W.item(), b.item(), cost.item()\n",
    "        ))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(2., requires_grad=True)\n",
      "수식을 w로 미분한 값 : 2.0\n",
      "수식을 w로 미분한 값 : 4.0\n",
      "수식을 w로 미분한 값 : 6.0\n",
      "수식을 w로 미분한 값 : 8.0\n",
      "수식을 w로 미분한 값 : 10.0\n",
      "수식을 w로 미분한 값 : 12.0\n",
      "수식을 w로 미분한 값 : 14.0\n",
      "수식을 w로 미분한 값 : 16.0\n",
      "수식을 w로 미분한 값 : 18.0\n",
      "수식을 w로 미분한 값 : 20.0\n",
      "수식을 w로 미분한 값 : 22.0\n",
      "수식을 w로 미분한 값 : 24.0\n",
      "수식을 w로 미분한 값 : 26.0\n",
      "수식을 w로 미분한 값 : 28.0\n",
      "수식을 w로 미분한 값 : 30.0\n",
      "수식을 w로 미분한 값 : 32.0\n",
      "수식을 w로 미분한 값 : 34.0\n",
      "수식을 w로 미분한 값 : 36.0\n",
      "수식을 w로 미분한 값 : 38.0\n",
      "수식을 w로 미분한 값 : 40.0\n",
      "수식을 w로 미분한 값 : 42.0\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "w = torch.tensor(2.0, requires_grad=True)\n",
    "print(w)\n",
    "\n",
    "nb_epochs = 20\n",
    "for epoch in range(nb_epochs + 1):\n",
    "    z = 2*w\n",
    "    z.backward()\n",
    "    print('수식을 w로 미분한 값 : {}'.format(w.grad))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1]\n",
      " [2]\n",
      " [3]\n",
      " [4]\n",
      " [5]]\n",
      "[[ 3]\n",
      " [ 5]\n",
      " [ 7]\n",
      " [ 9]\n",
      " [11]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "x_data1=np.array([1,2,3,4,5]).reshape(5,1)\n",
    "t_data1=np.array([3,5,7,9,11]).reshape(5,1)      # w = 2, b = 1 이 정답\n",
    "\n",
    "print(x_data1)\n",
    "print(t_data1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.],\n",
      "        [2.],\n",
      "        [3.],\n",
      "        [4.],\n",
      "        [5.]])\n",
      "tensor([[ 3.],\n",
      "        [ 5.],\n",
      "        [ 7.],\n",
      "        [ 9.],\n",
      "        [11.]])\n"
     ]
    }
   ],
   "source": [
    "x_data=torch.from_numpy(x_data1).float()\n",
    "t_data=torch.from_numpy(t_data1).float()\n",
    "print(x_data)\n",
    "print(t_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# data 활용하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>73</td>\n",
       "      <td>80</td>\n",
       "      <td>75</td>\n",
       "      <td>152</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>93</td>\n",
       "      <td>88</td>\n",
       "      <td>93</td>\n",
       "      <td>185</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>89</td>\n",
       "      <td>91</td>\n",
       "      <td>90</td>\n",
       "      <td>180</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>96</td>\n",
       "      <td>98</td>\n",
       "      <td>100</td>\n",
       "      <td>196</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>73</td>\n",
       "      <td>66</td>\n",
       "      <td>70</td>\n",
       "      <td>142</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>53</td>\n",
       "      <td>46</td>\n",
       "      <td>55</td>\n",
       "      <td>101</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>69</td>\n",
       "      <td>74</td>\n",
       "      <td>77</td>\n",
       "      <td>149</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>47</td>\n",
       "      <td>56</td>\n",
       "      <td>60</td>\n",
       "      <td>115</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>87</td>\n",
       "      <td>79</td>\n",
       "      <td>90</td>\n",
       "      <td>175</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>79</td>\n",
       "      <td>70</td>\n",
       "      <td>88</td>\n",
       "      <td>164</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>69</td>\n",
       "      <td>70</td>\n",
       "      <td>73</td>\n",
       "      <td>141</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>70</td>\n",
       "      <td>65</td>\n",
       "      <td>74</td>\n",
       "      <td>141</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>93</td>\n",
       "      <td>95</td>\n",
       "      <td>91</td>\n",
       "      <td>184</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>79</td>\n",
       "      <td>80</td>\n",
       "      <td>73</td>\n",
       "      <td>152</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>70</td>\n",
       "      <td>73</td>\n",
       "      <td>78</td>\n",
       "      <td>148</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>93</td>\n",
       "      <td>89</td>\n",
       "      <td>96</td>\n",
       "      <td>192</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>78</td>\n",
       "      <td>75</td>\n",
       "      <td>68</td>\n",
       "      <td>147</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>81</td>\n",
       "      <td>90</td>\n",
       "      <td>93</td>\n",
       "      <td>183</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>88</td>\n",
       "      <td>92</td>\n",
       "      <td>86</td>\n",
       "      <td>177</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>78</td>\n",
       "      <td>83</td>\n",
       "      <td>77</td>\n",
       "      <td>159</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>82</td>\n",
       "      <td>86</td>\n",
       "      <td>90</td>\n",
       "      <td>177</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>86</td>\n",
       "      <td>82</td>\n",
       "      <td>89</td>\n",
       "      <td>175</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>78</td>\n",
       "      <td>83</td>\n",
       "      <td>85</td>\n",
       "      <td>175</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>76</td>\n",
       "      <td>83</td>\n",
       "      <td>71</td>\n",
       "      <td>149</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>96</td>\n",
       "      <td>93</td>\n",
       "      <td>95</td>\n",
       "      <td>192</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     0   1    2    3\n",
       "0   73  80   75  152\n",
       "1   93  88   93  185\n",
       "2   89  91   90  180\n",
       "3   96  98  100  196\n",
       "4   73  66   70  142\n",
       "5   53  46   55  101\n",
       "6   69  74   77  149\n",
       "7   47  56   60  115\n",
       "8   87  79   90  175\n",
       "9   79  70   88  164\n",
       "10  69  70   73  141\n",
       "11  70  65   74  141\n",
       "12  93  95   91  184\n",
       "13  79  80   73  152\n",
       "14  70  73   78  148\n",
       "15  93  89   96  192\n",
       "16  78  75   68  147\n",
       "17  81  90   93  183\n",
       "18  88  92   86  177\n",
       "19  78  83   77  159\n",
       "20  82  86   90  177\n",
       "21  86  82   89  175\n",
       "22  78  83   85  175\n",
       "23  76  83   71  149\n",
       "24  96  93   95  192"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df=pd.read_csv('../data/data-01-test-score.csv', header=None)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 73  80  75]\n",
      " [ 93  88  93]\n",
      " [ 89  91  90]\n",
      " [ 96  98 100]\n",
      " [ 73  66  70]\n",
      " [ 53  46  55]\n",
      " [ 69  74  77]\n",
      " [ 47  56  60]\n",
      " [ 87  79  90]\n",
      " [ 79  70  88]\n",
      " [ 69  70  73]\n",
      " [ 70  65  74]\n",
      " [ 93  95  91]\n",
      " [ 79  80  73]\n",
      " [ 70  73  78]\n",
      " [ 93  89  96]\n",
      " [ 78  75  68]\n",
      " [ 81  90  93]\n",
      " [ 88  92  86]\n",
      " [ 78  83  77]\n",
      " [ 82  86  90]\n",
      " [ 86  82  89]\n",
      " [ 78  83  85]\n",
      " [ 76  83  71]\n",
      " [ 96  93  95]]\n",
      "[[ 73]\n",
      " [ 80]\n",
      " [ 75]\n",
      " [ 93]\n",
      " [ 88]\n",
      " [ 93]\n",
      " [ 89]\n",
      " [ 91]\n",
      " [ 90]\n",
      " [ 96]\n",
      " [ 98]\n",
      " [100]\n",
      " [ 73]\n",
      " [ 66]\n",
      " [ 70]\n",
      " [ 53]\n",
      " [ 46]\n",
      " [ 55]\n",
      " [ 69]\n",
      " [ 74]\n",
      " [ 77]\n",
      " [ 47]\n",
      " [ 56]\n",
      " [ 60]\n",
      " [ 87]\n",
      " [ 79]\n",
      " [ 90]\n",
      " [ 79]\n",
      " [ 70]\n",
      " [ 88]\n",
      " [ 69]\n",
      " [ 70]\n",
      " [ 73]\n",
      " [ 70]\n",
      " [ 65]\n",
      " [ 74]\n",
      " [ 93]\n",
      " [ 95]\n",
      " [ 91]\n",
      " [ 79]\n",
      " [ 80]\n",
      " [ 73]\n",
      " [ 70]\n",
      " [ 73]\n",
      " [ 78]\n",
      " [ 93]\n",
      " [ 89]\n",
      " [ 96]\n",
      " [ 78]\n",
      " [ 75]\n",
      " [ 68]\n",
      " [ 81]\n",
      " [ 90]\n",
      " [ 93]\n",
      " [ 88]\n",
      " [ 92]\n",
      " [ 86]\n",
      " [ 78]\n",
      " [ 83]\n",
      " [ 77]\n",
      " [ 82]\n",
      " [ 86]\n",
      " [ 90]\n",
      " [ 86]\n",
      " [ 82]\n",
      " [ 89]\n",
      " [ 78]\n",
      " [ 83]\n",
      " [ 85]\n",
      " [ 76]\n",
      " [ 83]\n",
      " [ 71]\n",
      " [ 96]\n",
      " [ 93]\n",
      " [ 95]]\n"
     ]
    }
   ],
   "source": [
    "data=df.values\n",
    "x_data=data[:,:3].reshape(-1, 3)\n",
    "y_data=data[:,:-1].reshape(-1, 1)\n",
    "print(x_data)\n",
    "print(y_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 73.,  80.,  75.],\n",
      "        [ 93.,  88.,  93.],\n",
      "        [ 89.,  91.,  90.],\n",
      "        [ 96.,  98., 100.],\n",
      "        [ 73.,  66.,  70.],\n",
      "        [ 53.,  46.,  55.],\n",
      "        [ 69.,  74.,  77.],\n",
      "        [ 47.,  56.,  60.],\n",
      "        [ 87.,  79.,  90.],\n",
      "        [ 79.,  70.,  88.],\n",
      "        [ 69.,  70.,  73.],\n",
      "        [ 70.,  65.,  74.],\n",
      "        [ 93.,  95.,  91.],\n",
      "        [ 79.,  80.,  73.],\n",
      "        [ 70.,  73.,  78.],\n",
      "        [ 93.,  89.,  96.],\n",
      "        [ 78.,  75.,  68.],\n",
      "        [ 81.,  90.,  93.],\n",
      "        [ 88.,  92.,  86.],\n",
      "        [ 78.,  83.,  77.],\n",
      "        [ 82.,  86.,  90.],\n",
      "        [ 86.,  82.,  89.],\n",
      "        [ 78.,  83.,  85.],\n",
      "        [ 76.,  83.,  71.],\n",
      "        [ 96.,  93.,  95.]])\n",
      "tensor([[ 73.],\n",
      "        [ 80.],\n",
      "        [ 75.],\n",
      "        [ 93.],\n",
      "        [ 88.],\n",
      "        [ 93.],\n",
      "        [ 89.],\n",
      "        [ 91.],\n",
      "        [ 90.],\n",
      "        [ 96.],\n",
      "        [ 98.],\n",
      "        [100.],\n",
      "        [ 73.],\n",
      "        [ 66.],\n",
      "        [ 70.],\n",
      "        [ 53.],\n",
      "        [ 46.],\n",
      "        [ 55.],\n",
      "        [ 69.],\n",
      "        [ 74.],\n",
      "        [ 77.],\n",
      "        [ 47.],\n",
      "        [ 56.],\n",
      "        [ 60.],\n",
      "        [ 87.],\n",
      "        [ 79.],\n",
      "        [ 90.],\n",
      "        [ 79.],\n",
      "        [ 70.],\n",
      "        [ 88.],\n",
      "        [ 69.],\n",
      "        [ 70.],\n",
      "        [ 73.],\n",
      "        [ 70.],\n",
      "        [ 65.],\n",
      "        [ 74.],\n",
      "        [ 93.],\n",
      "        [ 95.],\n",
      "        [ 91.],\n",
      "        [ 79.],\n",
      "        [ 80.],\n",
      "        [ 73.],\n",
      "        [ 70.],\n",
      "        [ 73.],\n",
      "        [ 78.],\n",
      "        [ 93.],\n",
      "        [ 89.],\n",
      "        [ 96.],\n",
      "        [ 78.],\n",
      "        [ 75.],\n",
      "        [ 68.],\n",
      "        [ 81.],\n",
      "        [ 90.],\n",
      "        [ 93.],\n",
      "        [ 88.],\n",
      "        [ 92.],\n",
      "        [ 86.],\n",
      "        [ 78.],\n",
      "        [ 83.],\n",
      "        [ 77.],\n",
      "        [ 82.],\n",
      "        [ 86.],\n",
      "        [ 90.],\n",
      "        [ 86.],\n",
      "        [ 82.],\n",
      "        [ 89.],\n",
      "        [ 78.],\n",
      "        [ 83.],\n",
      "        [ 85.],\n",
      "        [ 76.],\n",
      "        [ 83.],\n",
      "        [ 71.],\n",
      "        [ 96.],\n",
      "        [ 93.],\n",
      "        [ 95.]])\n"
     ]
    }
   ],
   "source": [
    "x_train=torch.from_numpy(x_data).float()\n",
    "y_train=torch.from_numpy(y_data).float()\n",
    "\n",
    "print(x_train)\n",
    "print(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.],\n",
      "        [0.],\n",
      "        [0.]], requires_grad=True)\n",
      "tensor([0.], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "W=torch.zeros((3,1), requires_grad=True)\n",
    "b=torch.zeros((1), requires_grad=True)\n",
    "print(W)\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "cost_list=[]\n",
    "optimizer=optim.SGD([W,b], lr=1e-6)\n",
    "epochs=30001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (25) must match the size of tensor b (75) at non-singleton dimension 0",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[72], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(epochs):\n\u001b[0;32m      2\u001b[0m     y\u001b[38;5;241m=\u001b[39mx_train\u001b[38;5;241m.\u001b[39mmatmul(W)\u001b[38;5;241m+\u001b[39mb\n\u001b[1;32m----> 3\u001b[0m     cost\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mmean((\u001b[43my\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43my_train\u001b[49m)\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m2\u001b[39m)\n\u001b[0;32m      4\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m      5\u001b[0m     cost\u001b[38;5;241m.\u001b[39mbackward()\n",
      "\u001b[1;31mRuntimeError\u001b[0m: The size of tensor a (25) must match the size of tensor b (75) at non-singleton dimension 0"
     ]
    }
   ],
   "source": [
    "for epoch in range(epochs):\n",
    "    y=x_train.matmul(W)+b\n",
    "    cost=torch.mean((y-y_train)**2)\n",
    "    optimizer.zero_grad()\n",
    "    cost.backward()\n",
    "    optimizer.step()\n",
    "    cost_list.append([epoch, cost])\n",
    "\n",
    "    if epoch%100 == 0:\n",
    "        print('epoch: ', epoch, 'cost: ', cost.item(), \"W: \", W, \"b: \", b.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(x):\n",
    "    return x.matmul(W)+b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.],\n",
       "        [0.],\n",
       "        [0.]], grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_data=torch.FloatTensor([[65,68,70], [100,95,90],[90,95,100]])\n",
    "predict(new_data)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
